- Every piece of code (such as a method) is associated
  with a function that indicates how many basic steps the code
  takes to do its job.
    - Examples of basic steps: arithmetic, comparisons, obtaining the value
      of a variable, assigning to a variable, obtaining value
      of an element in array via its index, assigning to an element
      of array, calling a method (plus the steps performed by the method).
    - Not basic steps: creating an array (takes arr.length steps),
      string concatenation (s1 + s2 takes s1.length() + s2.length() steps).
- This function is usually based on the size of the input,
  typically called n; for example, n may be the length of an array.
- This function tells us the code's "time complexity,"
  aka "running time," since the amount of time it will take the
  code to run is determined by the number of basic steps it must
  perform.
- These functions are usually difficult to determine precisely.
  But precision doesn't really matter to us here. We typically
  use Big Oh notation to simplify these functions.


Seven typical running times, with examples,
listed from best (fastest) to worst (slowest):
- constant: O(1) [the running time is totally uninfluenced by the input size]
  - accessing any element of an array based on its index
  - ArrayList's get and size methods
  - adding two numbers
  - finding the max of two numbers
- logarithmic: O(log n)
  - binary search (iterative and recursive)
    - Suppose we have 1,000,000 elements.
      only takes about 20 steps
- linear: O(n) [number of steps is proportional to n]
  - linear search
  - finding max of an array
  - ArrayList's contains method
- log-linear: n * log(n): O(n log n)
  - Java's Arrays.sort
  - merge sort
- quadratic: O(n^2)
  - selection sort
  - bubble sort
  - insertion sort
- cubic: O(n^3)
- exponential: O(2^n)

Here's a very informal definition of Big Oh notation
(formal definition in Analysis of Algorithms):
- A function f(n) is O(g(n)) if f(n) <= g(n) when
    - we ignore constant factors
    - we only focus on the most significant term.
- Examples:
    - n^2 is O(n^2) [every function is big Oh of itself]
    - Technically, n^2 is also O(n^3) and also O(n^4)
        - But we usually don't write this.
    - 100*n^2 + 45n - 3 is O(n^2)
    - n/4 is O(n), since n/4 = (1/4)*n
    - 156 is O(1), since 156 = 156 * 1
    - 100^100 is O(1)

We only care about what happens when n is large [asymptotic analysis].
Example: n^3 is O(2^n), even though n^3 <= 2^n is not true for
small values of n, such as n=3:
      n^3 = 3^3 = 27
      2^n = 2^3 = 8

Use the tightest possible bound:
- Technically, 3n is O(n^2).
- But we should use the big-Oh notation to characterize a
  function as closely as possible
- So we say that 3n is O(n).
- As another example, we said that n^3 is O(2^n), which is true,
  but we'd usually just write O(n^3)

Write big-oh notation as simply as possible:
- Technically, 3n + 15 is O(3n + 1).
- But we should write it as simply as possible.
- So we say that 3n + 15 is O(n).
- Similarly, we don't write things like O(100); we instead write O(1).

Example:
- Consider: program A's running time is 10n + 100,
  and program B's is n^2.
- Which is better? A, unless we're dealing with only small-sized inputs.

Big-Oh can sometimes be misleading.
- Consider: program C's running time is 1000000000n,
  and program D's is n log n.
- Just using big-Oh notation, C is better. But that would be a
  terrible choice for any reasonably-sized input.

Best case, worst case, and average case:
- The running time of a piece of code might depend on the state of
  the provided data.
- Example: linear search: the running time depends on where in the array
  the desired element is located.
    - In the best case, it's at the beginning: O(1)
    - In the worst case, it's at the end, or it doesn't exist: O(n)
    - On average, we have to look at half the elements: n/2, which is O(n)
- For many algorithms, we can differentiate between the best-case running time,
  the worst-case running time, and the average-case running time.
  - Best case is rarely useful to know.
  - Average case is usually difficult to determine.
  - So we typically focus on worst case.
- Example: selection sort: always O(n^2), even if the array is already sorted
  - Best case = worst case: running time of O(n^2)
- Example: for the optimized bubble sort, if the array comes fully sorted,
  then only one pass is performed, and we're done: O(n). In the worst case,
  we must perform all passes: O(n^2). Average turns out to be O(n^2).

Space complexity:
- Just as we can talk about the time complexity of a piece of code,
  we can talk about the space complexity of a piece of code:
  how much memory does it require?
- The memory used by the input (such as an array) doesn't count.
- We can use big-Oh notation for space complexity.

Logarithms:
- Log base b of n is approximately equal to the number of times
  that n can be divided by b until we reach 1 or lower.
- Log base b of n is  approximately equal to the number of times
  that x, starting at 1, can be multiplied by b until we reach n.
- In computer science, 2 is the most common logarithm base,
  so we write log base 2 of n as just log n.
- When using big-Oh notation, a log of any base can be simplified
  to log base 2, since logs of different bases differ only by a
  constant factor, due to the "change of base formula":
                  1
  log_b(n) = ---------- * log_d(n)
              log_d(b)